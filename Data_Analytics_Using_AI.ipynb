{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1**"
      ],
      "metadata": {
        "id": "tzoiNdd4PpDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIA7ad_sO3ei"
      },
      "outputs": [],
      "source": [
        "# Example: Categorizing analytics types\n",
        "analytics_types = {\n",
        "    \"Descriptive\": \"Understanding what happened\",\n",
        "    \"Diagnostic\": \"Understanding why it happened\",\n",
        "    \"Predictive\": \"Predicting what could happen\",\n",
        "    \"Prescriptive\": \"Suggesting what should be done\"\n",
        "}\n",
        "\n",
        "for key, value in analytics_types.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Comparing AI and Traditional Analytics\n",
        "traditional = \"Relies on predefined rules and algorithms.\"\n",
        "ai = \"Uses machine learning and adaptive algorithms.\"\n",
        "\n",
        "print(\"Traditional Analytics:\", traditional)\n",
        "print(\"AI Analytics:\", ai)\n"
      ],
      "metadata": {
        "id": "PLCvi71rPvn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lifecycle stages as a list\n",
        "lifecycle_stages = [\n",
        "    \"Problem Definition\",\n",
        "    \"Data Collection\",\n",
        "    \"Data Cleaning\",\n",
        "    \"Data Analysis\",\n",
        "    \"Data Interpretation\",\n",
        "    \"Reporting and Decision Making\"\n",
        "]\n",
        "\n",
        "for stage in lifecycle_stages:\n",
        "    print(stage)\n"
      ],
      "metadata": {
        "id": "VGLr9S_hPzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 2**"
      ],
      "metadata": {
        "id": "214xj5sKP4I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Listing internal and external data sources\n",
        "internal_sources = [\"Sales Database\", \"Customer Database\"]\n",
        "external_sources = [\"Public APIs\", \"Social Media Data\"]\n",
        "\n",
        "print(\"Internal Data Sources:\", internal_sources)\n",
        "print(\"External Data Sources:\", external_sources)\n"
      ],
      "metadata": {
        "id": "JmLzYq0aP2Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example: Handling Missing Values\n",
        "data = {'A': [1, 2, None], 'B': [4, None, 6]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Fill missing values\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "print(\"Cleaned Data:\\n\", df)\n"
      ],
      "metadata": {
        "id": "fyN_Xr5QP9AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Data Normalization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "\n",
        "print(\"Normalized Data:\\n\", normalized_data)\n"
      ],
      "metadata": {
        "id": "TNWFo9OxQAOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using AI for data preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Using SimpleImputer to handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputed_data = imputer.fit_transform(df)\n",
        "\n",
        "print(\"Imputed Data:\\n\", imputed_data)\n"
      ],
      "metadata": {
        "id": "8OqCTzgiQCol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 3**"
      ],
      "metadata": {
        "id": "1gu7SltqQP1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Basic statistics\n",
        "data = [1, 2, 3, 4, 5]\n",
        "mean = sum(data) / len(data)\n",
        "print(\"Mean:\", mean)\n"
      ],
      "metadata": {
        "id": "X5JPQG71QLzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: Creating a histogram\n",
        "plt.hist(data, bins=5)\n",
        "plt.title(\"Histogram\")\n",
        "plt.xlabel(\"Values\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8w9tURDQQWkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Automated EDA using pandas-profiling\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "profile = ProfileReport(df)\n",
        "profile.to_file(\"eda_report.html\")\n"
      ],
      "metadata": {
        "id": "0NViEYKkQZe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 4**"
      ],
      "metadata": {
        "id": "X_YFG10IQd8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Summary statistics using pandas\n",
        "summary_stats = df.describe()\n",
        "print(\"Summary Statistics:\\n\", summary_stats)\n"
      ],
      "metadata": {
        "id": "sVISNTqnQcdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Checking data distribution\n",
        "import seaborn as sns\n",
        "\n",
        "sns.histplot(data, kde=True)\n",
        "plt.title(\"Data Distribution with KDE\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2KIUD5ntQ549"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 5**"
      ],
      "metadata": {
        "id": "FkpynCrrRKht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Detecting anomalies using Z-score\n",
        "from scipy import stats\n",
        "\n",
        "z_scores = stats.zscore(df)\n",
        "print(\"Z-scores:\\n\", z_scores)\n"
      ],
      "metadata": {
        "id": "cvSS-rirRMrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: A/B Testing\n",
        "import scipy.stats as stats\n",
        "\n",
        "group_a = [20, 30, 50]\n",
        "group_b = [30, 20, 40]\n",
        "\n",
        "t_stat, p_value = stats.ttest_ind(group_a, group_b)\n",
        "print(\"T-statistic:\", t_stat, \"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "PbeL6A29RZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 6**"
      ],
      "metadata": {
        "id": "cClp5_w3RkRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Simple linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = [[1], [2], [3]]\n",
        "y = [1, 2, 3]\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "predictions = model.predict([[4]])\n",
        "print(\"Prediction for input 4:\", predictions)\n"
      ],
      "metadata": {
        "id": "ZCvtRPSURbiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Evaluating model using confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = [0, 1, 1, 0]\n",
        "y_pred = [0, 0, 1, 1]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "G1nIRYfVRp4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 7**"
      ],
      "metadata": {
        "id": "yXn2SyO5R4pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Linear programming with scipy\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# Minimize c @ x subject to Ax <= b\n",
        "c = [1, 2]\n",
        "A = [[-1, -1], [1, 2]]\n",
        "b = [-1, 6]\n",
        "\n",
        "result = linprog(c, A_ub=A, b_ub=b)\n",
        "print(\"Optimal value:\", result.fun, \"at x:\", result.x)\n"
      ],
      "metadata": {
        "id": "aRXpV8ZrR6g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 8**"
      ],
      "metadata": {
        "id": "lvTgkTQWSJ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: List of programming languages for data analytics\n",
        "languages = [\"Python\", \"R\", \"SQL\"]\n",
        "print(\"Programming Languages for Data Analytics:\", languages)\n"
      ],
      "metadata": {
        "id": "fD0Ptt-bSCla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 9**"
      ],
      "metadata": {
        "id": "q7aNt_iZS1Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sample Data\n",
        "data = pd.read_csv('finance_data.csv')  # Load your finance data\n",
        "X = data.drop('Risk_Label', axis=1)\n",
        "y = data['Risk_Label']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model Training\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "8rLwyKFwS3AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load healthcare data\n",
        "data = pd.read_csv('healthcare_data.csv')\n",
        "X = data.drop('Disease', axis=1)\n",
        "y = data['Disease']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model Training\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "mQxDW1IFS5ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load marketing data\n",
        "data = pd.read_csv('marketing_data.csv')\n",
        "X = data[['Age', 'Income']]\n",
        "\n",
        "# K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "data['Cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(data['Age'], data['Income'], c=data['Cluster'])\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Income')\n",
        "plt.title('Customer Segmentation')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GCUAlZJiS8sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# Sample Supply Chain Data\n",
        "c = [2, 3]  # Cost coefficients\n",
        "A = [[-1, -1], [1, 0], [0, 1]]  # Inequality coefficients\n",
        "b = [-5, 0, 0]  # Right-hand side\n",
        "\n",
        "# Linear Programming\n",
        "result = linprog(c, A_ub=A, b_ub=b)\n",
        "print('Optimal Solution:', result.x)\n"
      ],
      "metadata": {
        "id": "x8YEysR-TCAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('maintenance_data.csv')\n",
        "X = data.drop('Failure', axis=1)\n",
        "y = data['Failure']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model Training\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "sFt4d0QXTDIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Load transaction data\n",
        "data = pd.read_csv('transaction_data.csv')\n",
        "\n",
        "# Model for fraud detection\n",
        "model = IsolationForest(contamination=0.01)\n",
        "data['Fraud'] = model.fit_predict(data)\n",
        "\n",
        "# Fraud transactions\n",
        "fraud_transactions = data[data['Fraud'] == -1]\n"
      ],
      "metadata": {
        "id": "hs2vbe2hTHbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load customer data\n",
        "data = pd.read_csv('customer_data.csv')\n",
        "X = data.drop('Purchase_Amount', axis=1)\n",
        "y = data['Purchase_Amount']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model Training\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "986S284OTItw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 10**"
      ],
      "metadata": {
        "id": "3aSApTihTfTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = {'name': ['Alice', 'Bob', 'Charlie'], 'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Anonymizing email addresses\n",
        "df['email'] = df['email'].apply(lambda x: x.split('@')[0] + '@domain.com')\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "uAauo132TLfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sample user data\n",
        "user_data = {\n",
        "    'name': 'Alice',\n",
        "    'email': 'alice@example.com',\n",
        "    'preferences': {'newsletters': True, 'ads': False}\n",
        "}\n",
        "\n",
        "# Function to simulate a data access request\n",
        "def request_user_data(user_data):\n",
        "    return json.dumps(user_data, indent=2)\n",
        "\n",
        "print(request_user_data(user_data))\n"
      ],
      "metadata": {
        "id": "oY9fEW1BTkRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Sample predictions and true labels\n",
        "y_true = [0, 1, 1, 0, 1]\n",
        "y_pred = [0, 0, 1, 0, 1]\n",
        "\n",
        "# Confusion matrix to analyze bias\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n"
      ],
      "metadata": {
        "id": "nvFX6_fTTq_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "# Sample schema for ethical data usage\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"data_usage\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"marketing\", \"research\", \"internal\"]\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"data_usage\"]\n",
        "}\n",
        "\n",
        "# Sample data usage\n",
        "data_usage_info = {\"data_usage\": \"marketing\"}\n",
        "\n",
        "# Validate usage\n",
        "try:\n",
        "    validate(instance=data_usage_info, schema=schema)\n",
        "    print(\"Data usage policy is followed.\")\n",
        "except ValidationError as e:\n",
        "    print(\"Data usage policy violation:\", e.message)\n"
      ],
      "metadata": {
        "id": "CkmmsAg0Tr9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to log actions\n",
        "def log_action(action):\n",
        "    with open('audit_log.csv', mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([datetime.now(), action])\n",
        "\n",
        "# Example action\n",
        "log_action(\"User data accessed.\")\n"
      ],
      "metadata": {
        "id": "6x6-i6P4TvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ethical AI Guidelines\n",
        "1. **Fairness**: Ensure algorithms do not discriminate based on race, gender, or other attributes.\n",
        "2. **Transparency**: Provide clear explanations for AI decisions.\n",
        "3. **Accountability**: Establish clear responsibilities for AI outputs.\n",
        "4. **Privacy**: Ensure data is handled in compliance with GDPR and CCPA.\n"
      ],
      "metadata": {
        "id": "ytxYsVVJT0Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample biased dataset\n",
        "data = {'gender': ['male', 'female', 'female', 'male', 'female'], 'score': [1, 2, 3, 4, 2]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Mitigating bias by balancing classes\n",
        "df_balanced = df.groupby('gender').apply(lambda x: x.sample(df['gender'].value_counts().min(), random_state=1))\n",
        "print(df_balanced)\n"
      ],
      "metadata": {
        "id": "ZKDXTGgsTx8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample AI model performance metrics\n",
        "metrics = {\n",
        "    'model': ['Model A', 'Model B'],\n",
        "    'accuracy': [0.95, 0.90],\n",
        "    'fairness_score': [0.85, 0.80]\n",
        "}\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "# Analyzing performance\n",
        "ethical_practices = df_metrics[df_metrics['fairness_score'] < 0.85]\n",
        "print(\"Models needing improvement on fairness:\", ethical_practices)\n"
      ],
      "metadata": {
        "id": "q-SDvN6KT3qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data report\n",
        "data = {'metric': ['accuracy', 'precision', 'recall'], 'value': [0.95, 0.90, 0.85]}\n",
        "df_report = pd.DataFrame(data)\n",
        "\n",
        "# Generating a bar plot\n",
        "df_report.set_index('metric').plot(kind='bar')\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.savefig('model_performance_report.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-hVhxsqRT6Xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}